<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>3D Data Visualization Workshop</title><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Urbanist:wght@300;400;500;700&display=swap"><link rel=stylesheet href=/workshop-visualization/css/main.css><link rel=stylesheet href=/workshop-visualization/reveal/plugin/highlight/monokai.css><script>function toggleMetadata(e){const t=document.getElementById("metadata-"+e),n=event.target;t.style.display==="none"||t.style.display===""?t.style.display="flex":t.style.display="none"}</script><script>document.addEventListener("DOMContentLoaded",function(){const t=new URLSearchParams(window.location.search),n=t.get("view"),e=document.getElementById("page-content");if(n==="slides"){e.classList.add("reveal"),e.classList.remove("scroll");const n=e=>{const t=document.createElement("link");t.rel="stylesheet",t.href=e,document.head.appendChild(t)},t=(e,t)=>{const n=document.createElement("script");n.src=e,n.onload=t||function(){},document.body.appendChild(n)};n("/workshop-visualization/reveal/dist/reveal.css"),n("/workshop-visualization/reveal/dist/theme/white-hi.css"),n("/workshop-visualization/reveal/plugin/highlight/monokai.css"),t("/workshop-visualization/reveal/dist/reveal.js",function(){console.log("Reveal.js loaded"),t("/workshop-visualization/reveal/plugin/markdown/markdown.js",function(){t("/workshop-visualization/reveal/plugin/highlight/highlight.js",function(){t("/workshop-visualization/reveal/plugin/notes/notes.js",function(){console.log("All Reveal.js plugins loaded and ready to initialize."),typeof Reveal!="undefined"?Reveal.initialize({hash:!0,slideNumber:!0,center:!1,disableLayout:!0,transition:"fade",display:"flex",plugins:[RevealMarkdown,RevealNotes]}):console.error("Reveal.js is not defined, initialization failed.")})})})})}else e.classList.remove("reveal"),e.classList.add("scroll")})</script><script src=/workshop-visualization/js/jquery.min.js></script>
<script type=text/javascript src=/workshop-visualization/js/jquery.qrcode.js></script>
<script type=text/javascript src=/workshop-visualization/js/qrcode.js></script></head><body><div class=workshop id=page-content><div class=reveal-header><a href="?view=slides" class="toggle-view hidden-in-slides">View as Slides</a><div class=hi-icon></div></div><div class="slides limited-width" id=slides-container><section style=background-image:url(/workshop-visualization/img/bg.jpg)><div class=presentation-title><h1>3D Data Visualization Workshop</h1><div class=presenter><div>Deborah Schmidt</div><div>Head of Helmholtz Imaging Support Unit, MDC Berlin</div><div>Sep 25, 2024</div></div><img class=hidden-in-page src=/workshop-visualization/img/RZ_211119_Helmholtz-Imaging_Logo_4C_White.png height=90vh><aside class=notes>In this workshop, we highlight various approaches and methodologies for visualizing 3D datasets.</aside></div></section><section><nav class=toc><h2>Table of Contents</h2><ul><li><a href=#section-3>3D Dataset types</a></li><li><a href=#section-5>Visualizing volumetric datasets</a></li><li><a href=#section-11>Converting volumetric datasets into meshes</a></li><li><a href=#section-17>Mesh Processing</a></li><li><a href=#section-18>Rendering meshes</a></li><li><a href=#section-23>Choosing colors</a></li><li><a href=#section-24>Point Clouds - Project BESSY2 Reconstruction</a></li><li><a href=#section-25>Ongoing challenges and opportunities</a></li></ul></nav><div class=qr-code-slides><div id=qr-slides class=qr-code></div><script>jQuery("#qr-slides").qrcode({text:"https://ida-mdc.github.io/workshop-visualization/3d-data/"})</script><span>Slides available at <a href=https://ida-mdc.github.io/workshop-visualization/3d-data/>https://ida-mdc.github.io/workshop-visualization/3d-data/</a></span></div></section><section id=section-0><h2 id=introduction>Introduction</h2><p><unlisted><div class=flex></div><div class=horizontal><div><p>Hi, I&rsquo;m Deborah, head of the Helmholtz Imaging Support Unit at MDC.</p><h3 id=helmholtz-imaging-is-here-for-you-with-support-units-at-3-centers-working-in-close-collaboration-with-the-helmholtz-imaging-research-units>Helmholtz Imaging is here for you with Support Units at 3 centers, working in close collaboration with the Helmholtz Imaging Research Units.</h3><div class=logos><img src=https://ida-mdc.github.io/workshop-visualization/img/logos/desy.png>
<img src=https://ida-mdc.github.io/workshop-visualization/img/logos/dkfz.png>
<img src=https://ida-mdc.github.io/workshop-visualization/img/logos/mdc.png></div></div><figure><img src=https://ida-mdc.github.io/workshop-visualization/img/people/hi-support-staff.jpg alt="Members of all Helmholtz Imaging Support Units."><figcaption><p>Members of all Helmholtz Imaging Support Units.</p></figcaption></figure></div><div class=flex></div></p></section><section id=section-1><h2 id=introduction-1>Introduction</h2><unlisted><h3 id=consulting-along-the-entire-pipeline-and-across-all-research-domains>Consulting along the entire pipeline and across all research domains</h3><p><aside class=notes>You can contact us with any question along the imaging pipeline. We are funded to support all Helmholtz centers in
Germany, but you can also reach out to us if you don&rsquo;t belong to Helmholtz.</aside><div class=center><strong><a href=mailto:support@helmholtz-imaging.de>support@helmholtz-imaging.de</a></strong></div><figure><img src=https://ida-mdc.github.io/workshop-visualization/img/pipeline.png></figure></p></section><section id=section-2><h2 id=automation-of-rendering-tasks>Automation of rendering tasks</h2><unlisted><h3 id=album--the-image-challenges-catalog>Album & the Image Challenges catalog</h3><aside class=notes><p>According to feedback from the community, automating visualization tasks is often harder than other tasks, because
it often involves the use of graphical interfaces. Additionally, one often requires a whole list of tools to
complete a full workflow from preparing to rendering the data. My team works on a framework called <a href=https://album.solutions><strong>Album</strong></a> which can provide executable entry points into diverse tools, so
that they can be launched from the same interface and tailored to automate specific use cases.</p><p>Album manages separate virtual environments for each solution managed internally by micromamba to avoid version conflicts. Each solution can run custom installation and run scripts, written in Python, in these environments. Since Python runs across platforms, this enables us to write custom launchers and execution routines for a variety of software.</p><p>If you want to run any of the Album solutions mentioned in this workshop, please check out this tutorial on how to
install Album and how to add a catalog of solutions to your local album collection. Please install the following
catlog (as described in the tutorial):</p><div class=center><strong><code>https://gitlab.com/album-app/catalogs/image-challenges.git</code></strong></div></aside><ul><li><strong>Automate visualization tasks</strong>: Use Album to manage multiple tools from a single launcher.</li></ul><div class=tutorial><a class=tutorial-link href=https://ida-mdc.github.io/workshop-visualization/tutorial-album-user/><div><div class=tutorial-header><i>Separate tutorial - click this box!</i></div><div class=title><div id=qr-tutorial-album-user class=qr-code></div><script>jQuery("#qr-tutorial-album-user").qrcode({text:"https://ida-mdc.github.io/workshop-visualization/tutorial-album-user/"})</script><h3>How to install and use Album - a tool for decentralized software use case sharing</h3></div><div class=description>Did someone share an Album solution or catalog with you and you have never heard of the tool? This tutorial will summarize what Album does, and how you can install and use it.</div></div><span class=cover-image style=background-image:url(/workshop-visualization/img/album/everything-is-a-solution-dark.png)></span></a></div></section><section id=section-3><h2 id=3d-dataset-types>3D Dataset types</h2><div class=flex></div><div class=horizontal><div><aside class=notes><ul><li>Understanding the type of data you are working with is crucial for effective 3D rendering. Below, we outline the
common categories of datasets that are often visualized in three dimensions.</li></ul></aside><ul><li><strong>Voxel-Based Datasets</strong> (Euclidean-structured)</li></ul><aside class=notes>Voxel-based datasets represent data as a grid of values (voxels) in <strong>Euclidean space</strong>, where each voxel holds a specific value, such as intensity in medical imaging (e.g., CT, MRI scans) or simulation results. These datasets are inherently structured and can be visualized using volume rendering, isosurface extraction, or slice-based views.</aside><ul><li><strong>Meshes</strong> (Non-Euclidean-structured)</li></ul><aside class=notes>Meshes are composed of <strong>vertices, edges, and faces</strong> that define the surface of a 3D object. Unlike voxels, which describe volumes, meshes describe surfaces and are commonly used in <strong>computer graphics</strong> and <strong>CAD models</strong>. Meshes can be visualized through techniques like surface rendering, wireframe views, and texture mapping. Properties like curvature or scalar fields can also be mapped onto the mesh for enhanced interpretation.</aside><ul><li><strong>Point Clouds</strong> (Non-Euclidean-structured)</li></ul><aside class=notes>Point clouds are a collection of points in 3D space, where each point is defined by its <strong>X, Y, Z coordinates</strong>. They are often derived from <strong>LiDAR</strong> scans, 3D scanning, or particle simulations. Visualization of point clouds often involves <strong>point-based rendering</strong>, surface reconstruction, or filtering techniques to highlight areas of interest.</aside></div><figure><img src=https://ida-mdc.github.io/workshop-visualization/img/3d-data-representations.jpg alt="Examples of different 3D data type representations. Credit: Gezawa, A. et al. (2020), CC BY 4.0."><figcaption><p>Examples of different 3D data type representations. Credit: <a href=https://www.researchgate.net/publication/340074064_A_Review_on_Deep_Learning_Approaches_for_3D_Data_Representations_in_Retrieval_and_Classifications>Gezawa, A. et al. (2020), CC BY 4.0</a>.</p></figcaption></figure></div><div class=flex></div></section><section id=section-4 class=repeated-heading><h2 id=3d-dataset-types-1>3D Dataset types</h2><h3 id=voxel-based-images>Voxel-Based Images</h3><ul><li>Represent the <strong>entire volume</strong> of an object in a structured grid.</li><li>Each voxel holds a <strong>scalar value</strong>, often representing intensity in medical scans or simulation data.</li><li>Best for representing <strong>interior</strong> details of a structure, e.g., in <strong>CT/MRI scans</strong> or simulations.</li><li><strong>Volumetric rendering</strong> and <strong>slice-based views</strong> are common visualization techniques.</li></ul><figure><img src=https://ida-mdc.github.io/workshop-visualization/img/voxel-based-data.png alt="Voxel based data representation. Credit: Hasanov, S. et al. (2021), CC BY-SA 4.0."><figcaption><p>Voxel based data representation. Credit: <a href=https://www.researchgate.net/publication/353527450_Hierarchical_homogenization_and_experimental_evaluation_of_functionally_graded_materials_manufactured_by_the_fused_filament_fabrication_process>Hasanov, S. et al. (2021), CC BY-SA 4.0</a>.</p></figcaption></figure></section><section id=section-5><h2 id=visualizing-volumetric-datasets>Visualizing volumetric datasets</h2><aside class=notes>Volumetric datasets, such as medical imaging (CT/MRI scans) or fluid simulations, are complex datasets where each voxel represents a value in 3D space. In this section, we&rsquo;ll explore the various techniques available for rendering and interacting with volumetric data.</aside><ul><li><strong>Slice-Based Visualization</strong>: This involves rendering 2D cross-sections or &ldquo;slices&rdquo; of the 3D dataset, often used in
medical imaging.</li><li><strong>Volume raycasting</strong> (max intensity, emission absorbtion)</li></ul><div class=flex></div><div class=horizontal><figure><img src=https://ida-mdc.github.io/workshop-visualization/img/volume-rendering.png alt="Slicing, Max. Intensity, Emission Absorbtion"><figcaption><p>Slicing, Max. Intensity, Emission Absorbtion</p></figcaption></figure><figure><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/7/76/Volume_ray_casting.svg/2560px-Volume_ray_casting.svg.png alt="Thetawavederivative work: Florian Hofmann, CC BY-SA 3.0"><figcaption><p><a href="https://commons.wikimedia.org/w/index.php?curid=14521474">Thetawavederivative work: Florian Hofmann, CC BY-SA 3.0</a></p></figcaption></figure></div><div class=flex></div></section><section id=section-6 class=repeated-heading><h2 id=visualizing-volumetric-datasets-1>Visualizing volumetric datasets</h2><h3 id=transfer-functions>Transfer functions</h3><aside class=notes><p>When working with <strong>unannotated</strong> volumetric datasets, you can explore the data interactively using <strong>transfer functions</strong>. Transfer functions map intensity values in the dataset to colors and opacities, allowing you to visualize different regions of the volume without defining hard boundaries. This technique is often used for soft, exploratory visualizations of the internal structures of the data.</p><p><strong>Transfer Functions</strong>:</p><ul><li>A transfer function defines how data values are mapped to colors and transparency.</li><li>Example: Low intensity values may be mapped to transparent regions, while higher intensities are mapped to visible colors.</li><li>Transfer functions are typically adjusted in visualization software like <strong>ParaView</strong>.</li><li>By adjusting transfer functions, you can emphasize specific parts of the volume without needing concrete borders or segmentations.</li></ul></aside><figure><img src=https://www.researchgate.net/profile/Stefan-Bruckner-2/publication/227615609/figure/fig10/AS:1076152089206787@1633586060991/Illustrative-volume-rendering-using-a-style-transfer-function-Images-a-d-depict.png></figure><p>Figure by Stefan Bruckner from the following publication:</p><div style=flex:1></div><div class=citations><ul><li><a href=https://www.researchgate.net/publication/227615609_Style_Transfer_Functions_for_Illustrative_Volume_Rendering>Bruckner, Stefan & Gröller, Eduard. (2007). Style Transfer Functions for Illustrative Volume Rendering. Computer
Graphics Forum. 26. 715 - 724. 10.1111/j.1467-8659.2007.01095.x.</a></li></ul></div></section><section id=section-7 class=repeated-heading><h2 id=visualizing-volumetric-datasets-2>Visualizing volumetric datasets</h2><h3 id=volume-rendering-with-fiji-bigdataviewer-and-related-tools>Volume rendering with Fiji: BigDataViewer and related tools</h3><aside class=notes>Fiji is still choice number one for many who want to inspect an image quickly, mainly because it supports a vast
number of data formats. While Fiji can already render 3D data with its built in 3D Viewer, it also comes with
BigDataViewer (BDV), a great tool for arbitrary slicing of 3D data of any size. A whole ecosystem of tools based on BDV
has evolved over time, which we will explore in the tutorial linked below.</aside><ul><li><strong>Supports large data formats</strong>: BDV and Fiji can handle massive 3D datasets and allow arbitrary slicing.</li><li><strong>Ecosystem of tools</strong>: BDV serves as a foundation for other Fiji plugins that support multi-scale rendering and slicing.</li></ul><div class=tutorial><a class=tutorial-link href=https://ida-mdc.github.io/workshop-visualization/tutorial-volume-rendering-bdv/><div><div class=tutorial-header><i>Separate tutorial - click this box!</i></div><div class=title><div id=qr-tutorial-volume-rendering-bdv class=qr-code></div><script>jQuery("#qr-tutorial-volume-rendering-bdv").qrcode({text:"https://ida-mdc.github.io/workshop-visualization/tutorial-volume-rendering-bdv/"})</script><h3>Volume rendering with BigDataViewer tools</h3></div><div class=description>Learn how to render voxel-based volumetric data using BigDataViewer (BDV) and tools built on top of BDV.</div></div><span class=cover-image style=background-image:url(https://imagej.net/media/plugins/bdv/bdv-bdv-start.png)></span></a></div></section><section id=section-8 class=repeated-heading><h2 id=visualizing-volumetric-datasets-3>Visualizing volumetric datasets</h2><h3 id=volume-rendering-with-fiji-animation-with-3dscript>Volume rendering with Fiji: Animation with 3DScript</h3><p><aside class=notes>Even though we won&rsquo;t focus on animation in this workshop, I don&rsquo;t want to miss the opportunity to mention the
wonderful Fiji 3DScript plugin, which offers the ability to generate animations of 3D objects based using simple
text commands.</aside><div class=flex></div><div class=horizontal><div><ul><li><strong>Simple scripting</strong>: Create animations by writing basic scripts in natural language.</li><li><strong>Automated rendering</strong>: Generate complex 3D animations for presentations or publications.</li><li><a href=https://bene51.github.io/3Dscript/>Project website</a></li></ul></div><figure><img src=https://gitlab.com/album-app/catalogs/image-challenges/-/raw/visualization-animate-with-3dscript-0.1.2/solutions/visualization/animate-with-3dscript/cover.jpg></figure></div><div class=flex></div></p><div style=flex:1></div><div class=citations><ul><li><a href=https://www.nature.com/articles/s41592-019-0359-1>Schmid, B.; Tripal, P. & Fraaß, T. et al. (2019), &ldquo;3Dscript: animating 3D/4D microscopy data using a
natural-language-based syntax&rdquo;, Nature methods 16(4): 278-280,
PMID 30886414.</a></li></ul></div></section><section id=section-9 class=repeated-heading><h2 id=visualizing-volumetric-datasets-4>Visualizing volumetric datasets</h2><h3 id=python-based-tools>Python based tools</h3><aside class=notes>With the rise of popularity of Python as a script and programming language in the life sciences and beyond, let&rsquo;s
look at Python based volumetric rendering in the tutorial linked below. We will look at Napari, Pygfx, and VTK. It
is worth mentioning that he last two tools are also great resources for rendering mesh based datasets.</aside><div class=tutorial><a class=tutorial-link href=https://ida-mdc.github.io/workshop-visualization/tutorial-volume-rendering-python/><div><div class=tutorial-header><i>Separate tutorial - click this box!</i></div><div class=title><div id=qr-tutorial-volume-rendering-python class=qr-code></div><script>jQuery("#qr-tutorial-volume-rendering-python").qrcode({text:"https://ida-mdc.github.io/workshop-visualization/tutorial-volume-rendering-python/"})</script><h3>Volumetric Dataset Rendering in Python</h3></div><div class=description>A tutorial on python based tools for visualizing volumetric datasets in 3D, including napari, Pygfx, and VTK.</div></div><span class=cover-image style=background-image:url(/workshop-visualization/img/napari.png)></span></a></div></section><section id=section-10 class=repeated-heading><h2 id=visualizing-volumetric-datasets-5>Visualizing volumetric datasets</h2><h3 id=web-based-rendering-with-neuroglancer>Web based rendering with Neuroglancer</h3><aside class=notes>A web-based 3D viewer allows for interactive visualization directly in the browser without needing specialized software. These viewers can be embedded into web pages or shared with collaborators.
The following tutorial does not come with a full overview of existing web based viewers, but offers insight into a
project we are working on at MDC where we utilize Neuroglancer to display large scale mice brains online.</aside><ul><li><strong>Collaboration-friendly</strong>: Share URLs with collaborators to provide access to the 3D visualization.</li></ul><div class=tutorial><a class=tutorial-link href=https://ida-mdc.github.io/workshop-visualization/tutorial-volume-rendering-neuroglancer/><div><div class=tutorial-header><i>Separate tutorial - click this box!</i></div><div class=title><div id=qr-tutorial-volume-rendering-neuroglancer class=qr-code></div><script>jQuery("#qr-tutorial-volume-rendering-neuroglancer").qrcode({text:"https://ida-mdc.github.io/workshop-visualization/tutorial-volume-rendering-neuroglancer/"})</script><h3>Volumetric data rendering with Neuroglancer</h3></div><div class=description>Use case description of how to render voxel-based volumetric data using Neuroglancer and stream data locally or remotely for visualization.</div></div><span class=cover-image style=background-image:url(/workshop-visualization/img/neuroglancer.png)></span></a></div></section><section id=section-11><h2 id=converting-volumetric-datasets-into-meshes>Converting volumetric datasets into meshes</h2><h3 id=annotations>Annotations</h3><aside class=notes><p>Annotations can be used to add specific information to volumetric datasets, such as marking points of interest (e.g., cell locations, regions of interest) or segmenting areas of the data. Converting these annotated datasets into meshes allows for the visual representation of those specific features.</p><p>When working with <strong>unannotated</strong> volumetric datasets, you can explore the data interactively using <strong>transfer functions</strong>. Transfer functions map intensity values in the dataset to colors and opacities, allowing you to visualize different regions of the volume without defining hard boundaries. This technique is often used for soft, exploratory visualizations of the internal structures of the data.</p></aside><ul><li><strong>Transfer functions</strong>: Used for visualizing unannotated datasets, adjusting colors and opacities based on intensity values.</li></ul><aside class=notes>When converting volumetric data to <strong>meshes</strong>, it&rsquo;s necessary to draw concrete borders between the <strong>foreground</strong> (the object of interest) and the <strong>background</strong>. This is achieved through:</aside><ul><li><strong>Fixed thresholds</strong>: Used to generate meshes by separating foreground from background using a set intensity threshold.</li><li><strong>Content-based annotations</strong>: Create precise meshes by using annotated regions to define boundaries.</li></ul><div style=flex:1></div><div class=citations><ul><li><a href=https://rupress.org/jcb/article/220/2/e202010039/211599/3D-FIB-SEM-reconstruction-of-microtubule-organelle>© Müller et al. https://doi.org/10.1083/jcb.202010039</a></li></ul></div></section><section id=section-12 class=repeated-heading><h2 id=converting-volumetric-datasets-into-meshes-1>Converting volumetric datasets into meshes</h2><figure><img src=https://ida-mdc.github.io/workshop-visualization/img/annotation-conversion.jpg></figure></section><section id=section-13 class=repeated-heading><h2 id=converting-volumetric-datasets-into-meshes-2>Converting volumetric datasets into meshes</h2><h3 id=marching-cubes>Marching Cubes</h3><aside class=notes>The <strong>Marching Cubes algorithm</strong> is one of the most popular methods for extracting a 3D surface from volumetric data. It identifies the points in a voxel grid where the dataset crosses a specific threshold value (the <strong>isosurface</strong>) and uses those points to generate a mesh.</aside><figure><img src=https://ida-mdc.github.io/workshop-visualization/img/MarchingCubesEdit.svg alt="Marching cubes algorithm. Credit: Ryoshoru, Jmtrivial on Wikimedia, CC BY-SA 4.0" height=700px><figcaption><p>Marching cubes algorithm. Credit: <a href=https://commons.wikimedia.org/wiki/File:MarchingCubesEdit.svg>Ryoshoru, Jmtrivial on Wikimedia</a>, CC BY-SA 4.0</p></figcaption></figure></section><section id=section-14 class=repeated-heading><h2 id=converting-volumetric-datasets-into-meshes-3>Converting volumetric datasets into meshes</h2><h3 id=optimization>Optimization</h3><ul><li><strong>Binary masks</strong> vs. <strong>Probability maps</strong></li></ul><aside class=notes><p>When converting volumetric data to meshes, <strong>optimizing</strong> the output is crucial for achieving smooth and accurate
results. One good approach is using <strong>probability maps</strong> rather than binary masks as input for the <strong>Marching Cubes
algorithm</strong>.</p><ul><li><strong>Binary masks</strong>: Create rough, blocky meshes because the data is thresholded into hard 0/1 values, losing subpixel detail.</li><li><strong>Probability maps</strong>: Offer smoother results, as the algorithm can detect gradients between regions, improving mesh precision at subpixel levels.</li></ul></aside><figure><img src=https://ida-mdc.github.io/workshop-visualization/img/mesh-conversion-optimization.jpg></figure></section><section id=section-15 class=repeated-heading><h2 id=converting-volumetric-datasets-into-meshes-4>Converting volumetric datasets into meshes</h2><h3 id=reducing-mesh-complexity>Reducing mesh complexity</h3><aside class=notes>Large, complex meshes can be computationally intensive to render. Reducing mesh complexity helps with performance, especially for web viewers or real-time visualization. We’ll explore some standard techniques to simplify meshes while maintaining critical details.</aside><div class=flex></div><div class=horizontal><ul><li><strong>Decimation</strong>: A process to reduce the number of polygons in a mesh while maintaining the overall shape and detail.</li><li><strong>Remeshing</strong>: Tools like MeshLab and Blender offer remeshing techniques that can optimize mesh topology for better performance.</li><li><strong>LOD (Level of Detail)</strong>: Use LOD techniques to switch between different levels of mesh complexity based on the viewer’s distance.</li></ul><figure><img src=https://ida-mdc.github.io/workshop-visualization/img/reducing-mesh-complexity.jpg></figure></div><div class=flex></div></section><section id=section-16 class=repeated-heading><h2 id=converting-volumetric-datasets-into-meshes-5>Converting volumetric datasets into meshes</h2><h3 id=conversion-scripts>Conversion scripts</h3><aside class=notes>While several tools include converting volumetric datasets into meshes, VTK has worked particularly well in our
experience. Check out the tutorial below for more details. This includes Python code snippets, but also the
possibility to run conversion through a graphical user interface or command line using an Album solution.</aside><div class=tutorial><a class=tutorial-link href=https://ida-mdc.github.io/workshop-visualization/tutorial-mesh-conversion/><div><div class=tutorial-header><i>Separate tutorial - click this box!</i></div><div class=title><div id=qr-tutorial-mesh-conversion class=qr-code></div><script>jQuery("#qr-tutorial-mesh-conversion").qrcode({text:"https://ida-mdc.github.io/workshop-visualization/tutorial-mesh-conversion/"})</script><h3>Converting Volumes to Meshes with VTK</h3></div><div class=description>This tutorial will guide you through converting 3D pixel datasets, such as labelmaps and masks, into 3D meshes using VTK.</div></div><span class=cover-image style=background-image:url(/workshop-visualization/img/vtk-rendering.png)></span></a></div></section><section id=section-17><h2 id=mesh-processing>Mesh Processing</h2><aside class=notes>Once a mesh is generated from a volumetric dataset, further <strong>processing</strong> may be necessary to refine the mesh for better performance, rendering, or analysis. One of the most popular tools for mesh processing is <strong>MeshLab</strong>, an open-source application for cleaning, repairing, and optimizing 3D meshes.</aside><ul><li><p><strong>MeshLab</strong>: A powerful tool for cleaning, decimating, and refining 3D meshes. It supports:</p><ul><li><strong>Smoothing</strong>: Remove sharp edges or rough areas in the mesh.</li><li><strong>Decimation</strong>: Reduce the number of polygons while maintaining the overall shape.</li><li><strong>Repair</strong>: Fix holes or non-manifold geometry in the mesh for better usability.</li></ul></li><li><p><strong>Other tools</strong>: Blender and VTK also offer additional mesh processing capabilities.</p></li></ul></section><section id=section-18><div class=cover style=background-image:url(/workshop-visualization/img/single-betacell.jpg);background-color:#000;color:#fff><h2 style=color:#fff>Rendering meshes</h2><div style=flex:1></div><div style=flex:1></div><div class=citations><ul><li><a href=https://rupress.org/jcb/article/220/2/e202010039/211599/3D-FIB-SEM-reconstruction-of-microtubule-organelle>© Müller et al. https://doi.org/10.1083/jcb.202010039</a></li></ul></div></div><aside class=notes>Rendering meshes involves converting mesh data into visually meaningful images, taking into account surface
properties, lighting, and camera angles. Spending time on rendering approaches matching your use case is a way of
defining how to tell the story of your dataset.</aside></section><section id=section-19 class=repeated-heading><h2 id=rendering-meshes>Rendering meshes</h2><h3 id=rendering-pipeline>Rendering pipeline</h3><aside class=notes>In <strong>3D rendering</strong>, the graphics pipeline is responsible for transforming 3D coordinates into 2D pixels on the screen. This process involves several stages, where each step takes the output from the previous stage and prepares the data for the next. The pipeline efficiently transforms vertex data into pixels using <strong>shaders</strong>, small programs that run on the <strong>GPU</strong> to accelerate rendering. The pipeline can be divided into two main parts: <strong>geometry processing</strong> (converting 3D coordinates into 2D) and <strong>fragment processing</strong> (turning 2D data into colored pixels).</aside><div class=flex></div><div class=horizontal><ul><li><strong>Vertex Shader</strong>: Transforms 3D coordinates and applies basic vertex processing.</li><li><strong>Geometry Shader</strong> (optional): Generates new geometry (e.g., additional triangles) from existing primitives.</li><li><strong>Fragment Shader</strong>: Computes the final color of each pixel.</li><li><strong>Blending and Depth Testing</strong>: Determines how pixels are blended and which ones are visible.</li></ul><figure><img src=https://learnopengl.com/img/getting-started/pipeline.png alt="Credit: Joey de Vries,https://learnopengl.com/, CC BY 4.0"><figcaption><p>Credit: Joey de Vries,https://learnopengl.com/, CC BY 4.0</p></figcaption></figure></div><div class=flex></div><div style=flex:1></div><div class=citations><ul><li><a href=https://learnopengl.com/Getting-started/Hello-Triangle>Hello Triangle on learnopengl.com</a></li></ul></div></section><section id=section-20 class=repeated-heading><h2 id=rendering-meshes-1>Rendering meshes</h2><h3 id=rendering-meshes-with-vtk>Rendering meshes with VTK</h3><aside class=notes>VTK offers extensive tools for rendering meshes, allowing for the customization of surface properties and lighting to achieve the desired visualization. This tutorial will guide you through setting up a rendering pipeline in VTK, from loading meshes to final visualization.</aside><ul><li><strong>VTK rendering features</strong>: Customize surface properties like color, opacity, and lighting. VTK can also handle interactive rendering, where users can rotate and zoom in on the rendered mesh.</li></ul><div class=tutorial><a class=tutorial-link href=https://ida-mdc.github.io/workshop-visualization/tutorial-mesh-rendering-vtk/><div><div class=tutorial-header><i>Separate tutorial - click this box!</i></div><div class=title><div id=qr-tutorial-mesh-rendering-vtk class=qr-code></div><script>jQuery("#qr-tutorial-mesh-rendering-vtk").qrcode({text:"https://ida-mdc.github.io/workshop-visualization/tutorial-mesh-rendering-vtk/"})</script><h3>Rendering meshes in VTK</h3></div><div class=description>Learn how to render 3D meshes in VTK and get insights into the Python code behind it.</div></div><span class=cover-image style=background-image:url(/workshop-visualization/img/vtk-meshes.png)></span></a></div></section><section id=section-21 class=repeated-heading><h2 id=rendering-meshes-2>Rendering meshes</h2><h3 id=rendering-meshes-with-blender>Rendering meshes with Blender</h3><aside class=notes>Blender is a powerful open-source tool for rendering meshes. It supports realistic rendering, including lighting, shadows, transparency, and advanced surface textures. In this tutorial, you will learn how to set up Blender to render scientific datasets as meshes.</aside><div class=tutorial><a class=tutorial-link href=https://ida-mdc.github.io/workshop-visualization/tutorial-mesh-rendering-blender/><div><div class=tutorial-header><i>Separate tutorial - click this box!</i></div><div class=title><div id=qr-tutorial-mesh-rendering-blender class=qr-code></div><script>jQuery("#qr-tutorial-mesh-rendering-blender").qrcode({text:"https://ida-mdc.github.io/workshop-visualization/tutorial-mesh-rendering-blender/"})</script><h3>Rendering in Blender</h3></div><div class=description>In this section, we will cover specific Blender features to help you create beautiful renderings from scientific datasets. Blender is a powerful tool for this purpose, but it has a steep learning curve, so we’ll break it down step-by-step.</div></div><span class=cover-image style=background-image:url(/workshop-visualization/img/blender-render-result.png)></span></a></div></section><section id=section-22 class=repeated-heading><h2 id=rendering-meshes-3>Rendering meshes</h2><h3 id=cutting-volumes-in-blender>Cutting volumes in Blender</h3><aside class=notes><p>Blender’s powerful modeling and sculpting tools allow users to cut and manipulate 3D meshes. In this section, we&rsquo;ll cover techniques for cutting volumes to expose internal structures or focus on specific regions.</p><ul><li><strong>Cutting meshes</strong>: Use Blender’s <strong>Boolean modifier</strong> to subtract volumes and expose internal structures.</li><li><strong>Focus on regions</strong>: Cut specific parts of the mesh to highlight or reveal hidden features inside the object.</li></ul></aside><div class=tutorial><a class=tutorial-link href=https://ida-mdc.github.io/workshop-visualization/tutorial-mesh-cutting-volumes-blender/><div><div class=tutorial-header><i>Separate tutorial - click this box!</i></div><div class=title><div id=qr-tutorial-mesh-cutting-volumes-blender class=qr-code></div><script>jQuery("#qr-tutorial-mesh-cutting-volumes-blender").qrcode({text:"https://ida-mdc.github.io/workshop-visualization/tutorial-mesh-cutting-volumes-blender/"})</script><h3>Cutting mesh structures in Blender</h3></div><div class=description>This brief tutorial showcases how to cut into mesh objects in Blender to highlight otherwise hidden features of the scene. This can be particularly valuable for complex renderings of scientific datasets.</div></div><span class=cover-image style=background-image:url(/workshop-visualization/img/cutting-volumes.png)></span></a></div></section><section id=section-23><h2 id=choosing-colors>Choosing colors</h2><aside class=notes>Choosing the right colors for your 3D visualizations is crucial for enhancing clarity and understanding. The
tutorial below guides you through color representation in digital programs, a brief look into existing colormaps,
and a few more tricks for picking the best colors for your project.</aside><ul><li>Don&rsquo;t underestimate the impact of choosing colors matching your story!</li></ul><div class=tutorial><a class=tutorial-link href=https://ida-mdc.github.io/workshop-visualization/tutorial-choosing-colors/><div><div class=tutorial-header><i>Separate tutorial - click this box!</i></div><div class=title><div id=qr-tutorial-choosing-colors class=qr-code></div><script>jQuery("#qr-tutorial-choosing-colors").qrcode({text:"https://ida-mdc.github.io/workshop-visualization/tutorial-choosing-colors/"})</script><h3>Choosing colors for scientific 3D renderings</h3></div><div class=description>In this short tutorial, I will share tips and tricks for choosing colors for scientific 3D renderings.</div></div><span class=cover-image style=background-image:url(/workshop-visualization/img/colors.png)></span></a></div></section><section id=section-24><h2 id=point-clouds---project-bessy2-reconstruction>Point Clouds - Project BESSY2 Reconstruction</h2><div class=flex></div><div class=horizontal><h3 id=ongoing-helmholtz-imaging-collaboration-of-the-dkfz-support-unit-and-hzb-researchers>Ongoing Helmholtz Imaging Collaboration of the DKFZ Support Unit and HZB Researchers</h3><div><figure class=image-right><img src=https://ida-mdc.github.io/workshop-visualization/img/logos/dkfz.png height=50px></figure><figure class=image-right><img src=https://ida-mdc.github.io/workshop-visualization/img/logos/hzb-logo-a4-rgb.jpg height=80px></figure></div></div><div class=flex></div><aside class=notes>Experiments in BESSY II change regularly, making tracking those changes - e.g. for planning additional experiments - necessary. Common surveying techniques are laborious and offer unneeded accuracy. Thus, we provide a pragmatic solution where the status quo is reconstructed from drone video footage. The resulting 3D reconstruction can be rendered from above using orthogonal projection. Overlaying this rendering with the original 2D plans gives valuable information about the differences between the status quo and the theoretical plans.</aside><figure><img src=https://ida-mdc.github.io/workshop-visualization/img/bessy2.jpg></figure></section><section id=section-25><h2 id=ongoing-challenges-and-opportunities>Ongoing challenges and opportunities</h2><aside class=notes>In the future, we anticipate several areas of interest for 3D data visualization. These include advancements in web-based rendering tools, higher throughput for processing large datasets, and improvements in automation and dimensionality reduction.</aside><ul><li>Web based viewers</li><li>High Throughput</li><li>Automated workflows</li><li>Dimensionality reduction</li></ul><h3 id=would-you-be-interested-in-more-specific-tutorials>Would you be interested in more specific tutorials?</h3><ul><li>Animation</li><li>Different approaches to transparency in Blender</li><li>Point cloud handling</li><li>Time series</li></ul></section><section id=section-26><h2 id=thank-you>Thank you!</h2><p><unlisted><aside class=notes>Thank you for attending this workshop. Feel free to reach out for further assistance or to explore
more visualization techniques!</aside></p><div class=flex></div><div class=horizontal><div><h3 id=dont-hesitate-to-get-in-touch>Don&rsquo;t hesitate to get in touch!</h3><p><strong><a href=mailto:support@helmholtz-imaging.de>support@helmholtz-imaging.de</a></strong></p><p><strong><a href=https://connect.helmholtz-imaging.de>https://connect.helmholtz-imaging.de</a></strong></p><div class=flex></div><div class=logos><img src=https://ida-mdc.github.io/workshop-visualization/img/logos/desy.png>
<img src=https://ida-mdc.github.io/workshop-visualization/img/logos/dkfz.png>
<img src=https://ida-mdc.github.io/workshop-visualization/img/logos/mdc.png></div></div><figure><img src=https://ida-mdc.github.io/workshop-visualization/img/people/hi-support-staff.jpg alt="Helmholtz Imaging Support Units at DESY, DKFZ, and MDC."><figcaption><p>Helmholtz Imaging Support Units at DESY, DKFZ, and MDC.</p></figcaption></figure></div><div class=flex></div></section></div><div class=reveal-footer><span>2024</span> | <span><b>HELMHOLTZ IMAGING</b></span><span><span><a class=hidden-in-page href="?view=scroll"><strong>Exit slides</strong></a></span></span></div></div></body></html>